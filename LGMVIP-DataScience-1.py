# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qyKcGo5yAM0Mq60Tj36EMUhUp9C9NQSz

# Importing libraries
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

import warnings
warnings.filterwarnings('ignore')

df = pd.read_csv('iris_dataset.csv')

df.head()

df.tail()

"""# Checking for null values"""

df.isnull().sum()

df.info()

df.describe()

df['class'].value_counts()

"""# Encoding categorical variables"""

from sklearn.preprocessing import LabelEncoder

classes = pd.DataFrame({
    'class':['Iris-setosa','Iris-versicolor','Iris-virginica']
})

le = LabelEncoder()

df['class'] = le.fit_transform(df['class'])

df.head()

"""# Distribution of data"""

fig, axes = plt.subplots(2,2, figsize=(10,5), dpi=100)
fig.suptitle('Distribution of Sepal length, Sepal width, Petal length and Petal width')

sns.kdeplot(ax = axes[0,0], data=df, x='sepal length in cm', hue='class', alpha=0.5, shade=True)
axes[0,0].set_xlabel('Sepal length cm')
axes[0,0].get_legend().remove()

sns.kdeplot(ax=axes[0,1], data=df, x = 'sepal width in cm', hue='class', alpha=0.5, shade=True)
axes[0,1].set_label('Sepal width cm')
axes[0,1].get_legend().remove()

sns.kdeplot(ax=axes[1,0], data=df, x = 'petal length in cm', hue='class', alpha=0.5, shade=True)
axes[1,0].set_label('Petal length cm')
axes[1,0].get_legend().remove()

sns.kdeplot(ax=axes[1,1], data=df, x = 'petal width in cm', hue='class', alpha=0.5, shade=True)
axes[1,1].set_label('Petal width cm')

plt.tight_layout()

sns.heatmap(df.corr(),annot=True)

sns.pairplot(data=df, hue='class')

y = df['class']

X = df.drop('class',axis=1)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=8)

"""# Feature scaling"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

scaled_X_train = scaler.fit_transform(X_train)
scaled_X_test = scaler.transform(X_test)

scaled_X_train

"""# Model building"""

from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report

from sklearn.tree import DecisionTreeClassifier

dec = DecisionTreeClassifier()
dec.fit(scaled_X_train,y_train)

y_pred = dec.predict(scaled_X_test)

cm = confusion_matrix(y_test,y_pred,labels=dec.classes_)

sns.heatmap(cm,annot=True)

sns.heatmap(cm/np.sum(cm), annot=True,
            fmt='.2%', cmap='Blues')

acc_score = accuracy_score(y_test, y_pred)
round(acc_score, 3)

print(classification_report(y_test,y_pred))



from sklearn.linear_model import LogisticRegression
log = LogisticRegression()
log.fit(scaled_X_train,y_train)

y_pred = log.predict(scaled_X_test)

cm = confusion_matrix(y_test, y_pred,labels=log.classes_)

sns.heatmap(cm, annot=True)

sns.heatmap(cm/np.sum(cm), annot=True,
            fmt='.2%', cmap='Blues')

acc_score = accuracy_score(y_test, y_pred)
round(acc_score, 3)

print(classification_report(y_test, y_pred))

"""Decision Tree Classifier gave 0.87 while Logistic Regression gave  0.9. Hence LR works better in this dataset."""

